{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import fnmatch\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "\n",
    "    x = 0.5*(x+1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 3, x.size(-2),  x.size(-1))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# 可以使用高斯噪声，也可以使用均匀分布噪声，测试下来效果差不多\n",
    "# gaussian distribution and uniform distribution noises are both ok\n",
    "dist = {\n",
    "    'u':torch.rand,\n",
    "    'n':torch.randn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PersonDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, transform, inp_dim=416) -> None:\n",
    "        super(PersonDataset, self).__init__()\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.img_names = fnmatch.filter(os.listdir(self.img_dir), '*.png') + fnmatch.filter(os.listdir(self.img_dir), '*.jpg')\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inp_dim = inp_dim\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, self.img_names[index])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.pad_and_scale(image)\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def pad_and_scale(self, img):\n",
    "\n",
    "        w, h = img.size\n",
    "        if w == h:\n",
    "            padded_img = img\n",
    "        else:\n",
    "            dim_to_pad = 1 if w<h else 2\n",
    "            if dim_to_pad == 1:\n",
    "                padding = (h - w) / 2\n",
    "                padded_img = Image.new('RGB', (h, h), color=(127,127,127))\n",
    "                padded_img.paste(img, (int(padding), 0))\n",
    "            else:\n",
    "                padding = (w - h) / 2\n",
    "                padded_img = Image.new('RGB', (w, w), color=(127,127,127))\n",
    "                padded_img.paste(img, (0, int(padding)))\n",
    "        resize = transforms.Resize((self.inp_dim, self.inp_dim))\n",
    "        padded_img = resize(padded_img)     #choose here\n",
    "        \n",
    "        return padded_img\n",
    "\n",
    "\n",
    "# 定义不同尺度的自编码器\n",
    "# the autoencoder for different block size\n",
    "class AutoEncoder8(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder8, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=8, stride=2, padding=1),    # batch, 8, 24, 24\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=1),    # batch, 16, 12, 12\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1),    # batch, 32, 6, 6\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),    # batch, 16, 12, 12\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1),    # batch, 16, 24, 24\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 3, kernel_size=8, stride=2, padding=1),    # batch, 3, 52, 52\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class AutoEncoder16(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder16, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=2, stride=2, padding=1),    # batch, 8, 14, 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=2, stride=2, padding=1),    # batch, 16, 8, 8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=2, stride=2, padding=1),    # batch, 32, 5, 5\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2, padding=1),    # batch, 16, 8, 8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2, padding=1),    # batch, 8, 14, 14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 3, kernel_size=2, stride=2, padding=1),    # batch, 3, 26, 26\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class AutoEncoder32(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder32, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=2, stride=1, padding=1),    # batch, 8, 14, 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=2, stride=2, padding=1),    # batch, 16, 8, 8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=2, stride=2, padding=1),    # batch, 32, 5, 5\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2, padding=1),    # batch, 16, 8, 8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2, padding=1),    # batch, 8, 14, 14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 3, kernel_size=2, stride=1, padding=1),    # batch, 3, 13, 13\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练相关函数 Some functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于将噪声进行随机变换并加入到图像中\n",
    "# applying transformations to noise, and paste it to the image\n",
    "def noise_applier(img, noise):\n",
    "\n",
    "    noise = torch.clamp(noise, 0.000001, 0.999999)\n",
    "\n",
    "    pad_h = (img.size(-2)-noise.size(-2)) / 2\n",
    "    pad_w = (img.size(-1)-noise.size(-1)) / 2\n",
    "\n",
    "    mypad = nn.ConstantPad2d((int(pad_w+0.5), int(pad_w), int(pad_h+0.5), int(pad_h)), 0)\n",
    "    noise = mypad(noise)\n",
    "\n",
    "    scale_w = torch.FloatTensor(img.size(0)).uniform_(1.5,4).cuda()\n",
    "    scale_h = torch.FloatTensor(img.size(0)).uniform_(1.5,4).cuda()\n",
    "    rand_map = torch.rand((2, img.size(0))).cuda()\n",
    "    tx = ((rand_map[0,:]>0.66).float()-(rand_map[0,:]<0.33).float())*0.5\n",
    "    ty = ((rand_map[1,:]>0.66).float()-(rand_map[1,:]<0.33).float())*0.5\n",
    "    theta = torch.FloatTensor(img.size(0), 2, 3).fill_(0).cuda()\n",
    "\n",
    "    theta[:, 0, 0] = 1/scale_w\n",
    "    theta[:, 0, 1] = 0\n",
    "    theta[:, 0, 2] = tx/scale_w\n",
    "    theta[:, 1, 0] = 0\n",
    "    theta[:, 1, 1] = 1/scale_h\n",
    "    theta[:, 1, 2] = ty/scale_h\n",
    "\n",
    "    grid = nn.functional.affine_grid(theta, noise.shape)\n",
    "    noise_t = nn.functional.grid_sample(noise, grid)\n",
    "\n",
    "    img = torch.where((noise_t == 0), img, noise_t)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ae_trainer(model, train_loader, epochs, learning_rate, box_length, patch_distribution, pretrain=None, CUDA=True):\n",
    "\n",
    "    if pretrain != None:\n",
    "        model.load_state_dict(torch.load(pretrain))\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    noise_switch = 0    # 控制是否加入噪声   >=1: add noise 0: no\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for img in train_loader:\n",
    "            if CUDA:\n",
    "                img = img.cuda()\n",
    "\n",
    "            if noise_switch < 3:\n",
    "                output = model(img)\n",
    "                loss = criterion(output, img)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.data\n",
    "\n",
    "                noise_switch += 1\n",
    "            \n",
    "            else:\n",
    "\n",
    "                noise = dist[patch_distribution]((img.size(0), 3, box_length//4, box_length//4)).cuda()\n",
    "                #target_var = torch.var(noise, dim=(1,2,3))\n",
    "                img = noise_applier(img, noise)\n",
    "                output = model(img)\n",
    "                var_output = torch.var(output, dim=(1,2,3))\n",
    "                target_var = torch.ones_like(var_output)\n",
    "                loss = criterion(var_output, target_var)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.data\n",
    "\n",
    "                noise_switch = 0\n",
    "\n",
    "                #exit(1)\n",
    "\n",
    "        print('epoch [{}/{}], loss:{:.4f}'\n",
    "            .format(epoch, epochs-1, total_loss))\n",
    "\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            pic = to_img(img.cpu().data)\n",
    "            save_image(pic, \"./ae_train_process/{}_ori.png\".format(epoch))\n",
    "            pic = to_img(output.cpu().data)\n",
    "            save_image(pic, \"./ae_train_process/{}_rec.png\".format(epoch))\n",
    "        \n",
    "\n",
    "    if pretrain != None:\n",
    "        torch.save(model.state_dict(), pretrain)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), \"./ae_weights/%s_%d.pth\" % (patch_distribution, box_length))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [0/1], loss:0.3016\n",
      "epoch [1/1], loss:0.3588\n"
     ]
    }
   ],
   "source": [
    "# uniform distribution\n",
    "learning_rate = 0.002\n",
    "epochs = 2 # 2 is for testing the code, using 150 works better\n",
    "box_length = 13\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize(size=400),\n",
    "    transforms.RandomCrop(box_length),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "img_dir = \"Dataset/back_dhd_coco/\"\n",
    "batch_size = 200\n",
    "dataset_train = PersonDataset(img_dir, transform)\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = AutoEncoder32().cuda()\n",
    "\n",
    "ae_trainer(model, train_loader, epochs, learning_rate, box_length, patch_distribution='u', pretrain=None, CUDA=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [0/1], loss:0.2460\n",
      "epoch [1/1], loss:0.2822\n"
     ]
    }
   ],
   "source": [
    "# gausion distribution\n",
    "learning_rate = 0.005\n",
    "epochs = 2 # 2 is for testing the code, using 150 works better\n",
    "box_length = 13\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(box_length),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "img_dir = \"Dataset/back_dhd_coco/\"\n",
    "batch_size = 200\n",
    "dataset_train = PersonDataset(img_dir, transform)\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = AutoEncoder32().cuda()\n",
    "\n",
    "ae_trainer(model, train_loader, epochs, learning_rate, box_length, patch_distribution='n', CUDA=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae3b0402f9dd51eaa3f0a02741215eae93554903d09a4648ec36843d0772e640"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
