{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from darknet_v2 import Darknet\n",
    "from util_model_img import load_classes, letterbox_image, image_to_tensor, predict_transform_v2, write_results\n",
    "\n",
    "dataset = \"coco\"\n",
    "stride = 32\n",
    "confidence = 0.5\n",
    "nms_thresh = 0.4\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "inp_dim = 416\n",
    "num_classes = 80\n",
    "classes = load_classes('data/coco.names')\n",
    "weightsfile = 'model_weights/yolo.weights'\n",
    "cfgfile = \"cfg/yolo.cfg\"\n",
    "\n",
    "model = Darknet(cfgfile)\n",
    "model.load_weights(weightsfile)\n",
    "if CUDA:\n",
    "    model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "\n",
    "def detection(val_anno, img_dir, results_json, class_id, fps=False):\n",
    "\n",
    "    '''\n",
    "    val_anno: annotations of validation set images\n",
    "    img_dir: dir of validation set images\n",
    "    results_json: detection result\n",
    "    class_id: HA target class - person (0) and AA target class - stop sign (11)\n",
    "    '''\n",
    "\n",
    "    file2id = dict()\n",
    "    with open(val_anno) as f:\n",
    "        dic = json.load(f)\n",
    "        images = dic['images']\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            file2id[image['file_name']] = image['id']\n",
    "\n",
    "    #imgs = glob.glob(\"%s/*.png\" % img_dir) + glob.glob(\"%s/*.jpg\" % img_dir)\n",
    "    imgs = [f for f in os.listdir(img_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    if fps:\n",
    "        start = time.time()\n",
    "\n",
    "    for img in imgs:\n",
    "\n",
    "        image_id = file2id[img]\n",
    "\n",
    "        img_cv = cv2.imread(os.path.join(img_dir, img))\n",
    "        img_h, img_w, _ = img_cv.shape\n",
    "        img_cv = letterbox_image(img_cv, [inp_dim, inp_dim])\n",
    "        im_dim = torch.FloatTensor((img_w, img_h)).repeat(1,2)\n",
    "        img_ts = image_to_tensor(img_cv)\n",
    "\n",
    "        if CUDA:\n",
    "            img_ts = img_ts.cuda()\n",
    "            im_dim = im_dim.cuda()\n",
    "        \n",
    "        prediction = model(img_ts).data\n",
    "        prediction = predict_transform_v2(prediction, inp_dim, model.anchors, num_classes, stride, confidence, CUDA)\n",
    "        output = write_results(prediction, confidence, num_classes, nms=True, nms_conf=nms_thresh)\n",
    "\n",
    "        if type(output) != int:\n",
    "            im_dim = im_dim.repeat(output.size(0), 1)\n",
    "            scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1).cuda()\n",
    "            output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n",
    "            output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n",
    "            output[:,1:5] /= scaling_factor\n",
    "            for i in range(output.shape[0]):\n",
    "                output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n",
    "                output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n",
    "                conf = output[i, 5].float().item()\n",
    "                cls = output[i, -1].int().item()\n",
    "                x = output[i, 1].int().item()\n",
    "                y = output[i, 2].int().item()\n",
    "                w = (output[i, 3] - output[i, 1]).float().item()\n",
    "                h = (output[i, 4] - output[i, 2]).float().item()\n",
    "\n",
    "                #print(image_id, cls, [xc, yc, w, h], conf)\n",
    "\n",
    "                results.append({'image_id': image_id,\n",
    "                                'category_id': cls,\n",
    "                                'bbox': [x, y, w, h],\n",
    "                                'score': conf})\n",
    "        \n",
    "    if fps:\n",
    "        end = time.time()\n",
    "        total_time = end-start\n",
    "        print(\"%d imgs, %f seconds, average: %f fps\" % (len(imgs), total_time, len(imgs)/total_time))\n",
    "\n",
    "    with open(results_json, 'w') as f:\n",
    "        f.write(json.dumps(results, indent=4))\n",
    "\n",
    "\n",
    "    cocoGt = COCO(val_anno)\n",
    "    cocoDt = cocoGt.loadRes(results_json)\n",
    "    cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "    cocoEval.params.catIds = [class_id]\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INRIA 数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 imgs, 10.149807 seconds, average: 28.374923 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.10s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.454\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.847\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.466\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.217\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.498\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.312\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.540\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.540\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.315\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.587\n"
     ]
    }
   ],
   "source": [
    "detection(\n",
    "    val_anno=\"Dataset/HA/test_annotations.json\",\n",
    "    img_dir=\"Dataset/HA/INRIA/pos/\",\n",
    "    results_json=\"test_map/yolov2_inria_test.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Adv Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 imgs, 6.683018 seconds, average: 43.094304 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.08s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.089\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.240\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.097\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.110\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.093\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.161\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.161\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.042\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.169\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.162\n"
     ]
    }
   ],
   "source": [
    "\n",
    "detection(\n",
    "    val_anno=\"Dataset/HA/test_adv_annotations.json\",\n",
    "    img_dir=\"Dataset/HA/INRIA/patch_v2/\",\n",
    "    results_json=\"test_map/yolov2_inria_patchv2.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adv Cloak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 imgs, 6.374250 seconds, average: 45.181783 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.136\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.330\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.085\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.125\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.185\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.140\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.215\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.215\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.026\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.241\n"
     ]
    }
   ],
   "source": [
    "\n",
    "detection(\n",
    "    val_anno=\"Dataset/HA/test_adv_annotations.json\",\n",
    "    img_dir=\"Dataset/HA/INRIA/cloak_v2/\",\n",
    "    results_json=\"test_map/yolov2_inria_cloakv2.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adv Tshirt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 imgs, 6.500369 seconds, average: 44.305178 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.173\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.399\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.109\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.125\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.262\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.261\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.214\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.332\n"
     ]
    }
   ],
   "source": [
    "\n",
    "detection(\n",
    "    val_anno=\"Dataset/HA/test_adv_annotations.json\",\n",
    "    img_dir=\"Dataset/HA/INRIA/tshirt_v2/\",\n",
    "    results_json=\"test_map/yolov2_inria_tshirtv2.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 imgs, 6.785481 seconds, average: 42.443563 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.08s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.216\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.476\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.153\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.261\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.300\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.300\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.021\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.281\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.342\n"
     ]
    }
   ],
   "source": [
    "\n",
    "detection(\n",
    "    val_anno=\"Dataset/HA/test_adv_annotations.json\",\n",
    "    img_dir=\"Dataset/HA/INRIA/natural_v2/\",\n",
    "    results_json=\"test_map/yolov2_inria_naturalv2.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 imgs, 26.070091 seconds, average: 38.358132 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.38s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.631\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.900\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.805\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.620\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.637\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.668\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.668\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.668\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.669\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.673\n"
     ]
    }
   ],
   "source": [
    "detection(\n",
    "    val_anno=\"Dataset/AA/stop_test_annotations.json\",\n",
    "    img_dir=\"Dataset/AA/imgs_s/\",\n",
    "    results_json=\"test_map/yolov2_aa_stopsign.json\",\n",
    "    class_id=11,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 imgs, 24.380700 seconds, average: 41.016050 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.22s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.06s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.545\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.972\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.580\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.455\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.625\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.592\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.598\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.598\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.509\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.688\n"
     ]
    }
   ],
   "source": [
    "detection(\n",
    "    val_anno=\"Dataset/AA/person_test_annotations.json\",\n",
    "    img_dir=\"Dataset/AA/imgs_p/\",\n",
    "    results_json=\"test_map/yolov2_aa_person.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Darknet(\n",
       "  (module_list): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (conv_0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_0): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (conv_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (conv_2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (conv_3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (shortcut_4): EmptyLayer()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (conv_5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batch_norm_5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (conv_6): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (conv_7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_7): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (shortcut_8): EmptyLayer()\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (conv_9): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_9): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (conv_10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_10): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (shortcut_11): EmptyLayer()\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (conv_12): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batch_norm_12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_12): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (conv_13): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_13): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (conv_14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_14): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (shortcut_15): EmptyLayer()\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (conv_16): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_16): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_16): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (conv_17): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_17): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (shortcut_18): EmptyLayer()\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (conv_19): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_19): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_19): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (conv_20): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_20): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (21): Sequential(\n",
       "      (shortcut_21): EmptyLayer()\n",
       "    )\n",
       "    (22): Sequential(\n",
       "      (conv_22): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_22): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (conv_23): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_23): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (24): Sequential(\n",
       "      (shortcut_24): EmptyLayer()\n",
       "    )\n",
       "    (25): Sequential(\n",
       "      (conv_25): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_25): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (conv_26): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_26): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (27): Sequential(\n",
       "      (shortcut_27): EmptyLayer()\n",
       "    )\n",
       "    (28): Sequential(\n",
       "      (conv_28): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_28): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (29): Sequential(\n",
       "      (conv_29): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_29): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_29): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (30): Sequential(\n",
       "      (shortcut_30): EmptyLayer()\n",
       "    )\n",
       "    (31): Sequential(\n",
       "      (conv_31): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_31): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_31): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (32): Sequential(\n",
       "      (conv_32): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_32): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_32): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (33): Sequential(\n",
       "      (shortcut_33): EmptyLayer()\n",
       "    )\n",
       "    (34): Sequential(\n",
       "      (conv_34): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_34): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_34): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (35): Sequential(\n",
       "      (conv_35): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_35): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_35): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (36): Sequential(\n",
       "      (shortcut_36): EmptyLayer()\n",
       "    )\n",
       "    (37): Sequential(\n",
       "      (conv_37): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batch_norm_37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_37): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (38): Sequential(\n",
       "      (conv_38): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_38): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_38): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (39): Sequential(\n",
       "      (conv_39): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_39): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_39): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (40): Sequential(\n",
       "      (shortcut_40): EmptyLayer()\n",
       "    )\n",
       "    (41): Sequential(\n",
       "      (conv_41): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_41): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_41): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (42): Sequential(\n",
       "      (conv_42): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_42): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_42): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (43): Sequential(\n",
       "      (shortcut_43): EmptyLayer()\n",
       "    )\n",
       "    (44): Sequential(\n",
       "      (conv_44): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_44): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_44): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (45): Sequential(\n",
       "      (conv_45): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_45): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_45): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (46): Sequential(\n",
       "      (shortcut_46): EmptyLayer()\n",
       "    )\n",
       "    (47): Sequential(\n",
       "      (conv_47): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_47): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_47): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (48): Sequential(\n",
       "      (conv_48): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_48): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_48): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (49): Sequential(\n",
       "      (shortcut_49): EmptyLayer()\n",
       "    )\n",
       "    (50): Sequential(\n",
       "      (conv_50): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_50): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_50): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (51): Sequential(\n",
       "      (conv_51): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_51): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_51): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (52): Sequential(\n",
       "      (shortcut_52): EmptyLayer()\n",
       "    )\n",
       "    (53): Sequential(\n",
       "      (conv_53): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_53): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_53): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (54): Sequential(\n",
       "      (conv_54): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_54): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_54): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (55): Sequential(\n",
       "      (shortcut_55): EmptyLayer()\n",
       "    )\n",
       "    (56): Sequential(\n",
       "      (conv_56): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_56): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_56): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (57): Sequential(\n",
       "      (conv_57): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_57): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_57): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (58): Sequential(\n",
       "      (shortcut_58): EmptyLayer()\n",
       "    )\n",
       "    (59): Sequential(\n",
       "      (conv_59): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_59): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_59): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (60): Sequential(\n",
       "      (conv_60): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_60): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_60): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (61): Sequential(\n",
       "      (shortcut_61): EmptyLayer()\n",
       "    )\n",
       "    (62): Sequential(\n",
       "      (conv_62): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batch_norm_62): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_62): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (63): Sequential(\n",
       "      (conv_63): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_63): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_63): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (64): Sequential(\n",
       "      (conv_64): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_64): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_64): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (65): Sequential(\n",
       "      (shortcut_65): EmptyLayer()\n",
       "    )\n",
       "    (66): Sequential(\n",
       "      (conv_66): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_66): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_66): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (67): Sequential(\n",
       "      (conv_67): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_67): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_67): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (68): Sequential(\n",
       "      (shortcut_68): EmptyLayer()\n",
       "    )\n",
       "    (69): Sequential(\n",
       "      (conv_69): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_69): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_69): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (70): Sequential(\n",
       "      (conv_70): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_70): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_70): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (71): Sequential(\n",
       "      (shortcut_71): EmptyLayer()\n",
       "    )\n",
       "    (72): Sequential(\n",
       "      (conv_72): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_72): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_72): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (73): Sequential(\n",
       "      (conv_73): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_73): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_73): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (74): Sequential(\n",
       "      (shortcut_74): EmptyLayer()\n",
       "    )\n",
       "    (75): Sequential(\n",
       "      (conv_75): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_75): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_75): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (76): Sequential(\n",
       "      (conv_76): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_76): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_76): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (77): Sequential(\n",
       "      (conv_77): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_77): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_77): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (78): Sequential(\n",
       "      (conv_78): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_78): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_78): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (79): Sequential(\n",
       "      (conv_79): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_79): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_79): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (80): Sequential(\n",
       "      (conv_80): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_80): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_80): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (81): Sequential(\n",
       "      (conv_81): Conv2d(1024, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (82): Sequential(\n",
       "      (Detection_82): DetectionLayer()\n",
       "    )\n",
       "    (83): Sequential(\n",
       "      (route_83): EmptyLayer()\n",
       "    )\n",
       "    (84): Sequential(\n",
       "      (conv_84): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_84): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_84): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (85): Sequential(\n",
       "      (upsample_85): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    )\n",
       "    (86): Sequential(\n",
       "      (route_86): EmptyLayer()\n",
       "    )\n",
       "    (87): Sequential(\n",
       "      (conv_87): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_87): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_87): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (88): Sequential(\n",
       "      (conv_88): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_88): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_88): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (89): Sequential(\n",
       "      (conv_89): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_89): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_89): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (90): Sequential(\n",
       "      (conv_90): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_90): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_90): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (91): Sequential(\n",
       "      (conv_91): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_91): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_91): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (92): Sequential(\n",
       "      (conv_92): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_92): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_92): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (93): Sequential(\n",
       "      (conv_93): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (94): Sequential(\n",
       "      (Detection_94): DetectionLayer()\n",
       "    )\n",
       "    (95): Sequential(\n",
       "      (route_95): EmptyLayer()\n",
       "    )\n",
       "    (96): Sequential(\n",
       "      (conv_96): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_96): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_96): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (97): Sequential(\n",
       "      (upsample_97): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    )\n",
       "    (98): Sequential(\n",
       "      (route_98): EmptyLayer()\n",
       "    )\n",
       "    (99): Sequential(\n",
       "      (conv_99): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_99): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_99): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (100): Sequential(\n",
       "      (conv_100): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_100): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_100): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (101): Sequential(\n",
       "      (conv_101): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_101): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_101): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (102): Sequential(\n",
       "      (conv_102): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_102): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_102): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (103): Sequential(\n",
       "      (conv_103): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_103): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_103): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (104): Sequential(\n",
       "      (conv_104): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_104): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_104): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (105): Sequential(\n",
       "      (conv_105): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (106): Sequential(\n",
       "      (Detection_106): DetectionLayer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from darknet_v3 import Darknet as Darknet53\n",
    "from util_model_img import load_classes, letterbox_image, image_to_tensor, write_result\n",
    "\n",
    "confidence = 0.5\n",
    "nms_thresh = 0.4\n",
    "num_classes = 80\n",
    "inp_dim = 416\n",
    "device=\"cuda\"\n",
    "device = torch.device(device)\n",
    "weightsfile = \"model_weights/yolov3.weights\"\n",
    "cfgfile = \"cfg/yolov3.cfg\"\n",
    "\n",
    "model = Darknet53(device, cfgfile)\n",
    "model.load_weights(weightsfile)\n",
    "model.net_info[\"height\"] = inp_dim\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "\n",
    "def detection_v3(val_anno, img_dir, results_json, class_id, CUDA=True, fps=False):\n",
    "\n",
    "    '''\n",
    "    val_anno: annotations of validation set images\n",
    "    img_dir: dir of validation set images\n",
    "    results_json: detection result\n",
    "    class_id: HA target class - person (0) and AA target class - stop sign (11)\n",
    "    '''\n",
    "\n",
    "    file2id = dict()\n",
    "    with open(val_anno) as f:\n",
    "        dic = json.load(f)\n",
    "        images = dic['images']\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            file2id[image['file_name']] = image['id']\n",
    "\n",
    "    #imgs = glob.glob(\"%s/*.png\" % img_dir) + glob.glob(\"%s/*.jpg\" % img_dir)\n",
    "    imgs = [f for f in os.listdir(img_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    if fps:\n",
    "        start = time.time()\n",
    "\n",
    "    for img in imgs:\n",
    "\n",
    "        image_id = file2id[img]\n",
    "\n",
    "        img_cv = cv2.imread(os.path.join(img_dir, img))\n",
    "        img_h, img_w, _ = img_cv.shape\n",
    "        img_cv = letterbox_image(img_cv, [inp_dim, inp_dim])\n",
    "        im_dim = torch.FloatTensor((img_w, img_h)).repeat(1,2)\n",
    "        img_ts = image_to_tensor(img_cv)\n",
    "\n",
    "        if CUDA:\n",
    "            img_ts = img_ts.cuda()\n",
    "            im_dim = im_dim.cuda()\n",
    "        \n",
    "        prediction = model(img_ts).data\n",
    "        output = write_result(prediction, confidence, num_classes, nms=True, nms_conf=nms_thresh)\n",
    "\n",
    "        if type(output) != int:\n",
    "            im_dim = im_dim.repeat(output.size(0), 1)\n",
    "            scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1).cuda()\n",
    "            output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n",
    "            output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n",
    "            output[:,1:5] /= scaling_factor\n",
    "            for i in range(output.shape[0]):\n",
    "                output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n",
    "                output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n",
    "                conf = output[i, 5].float().item()\n",
    "                cls = output[i, -1].int().item()\n",
    "                x = output[i, 1].int().item()\n",
    "                y = output[i, 2].int().item()\n",
    "                w = (output[i, 3] - output[i, 1]).float().item()\n",
    "                h = (output[i, 4] - output[i, 2]).float().item()\n",
    "\n",
    "                #print(image_id, cls, [xc, yc, w, h], conf)\n",
    "\n",
    "                results.append({'image_id': image_id,\n",
    "                                'category_id': cls,\n",
    "                                'bbox': [x, y, w, h],\n",
    "                                'score': conf})\n",
    "        \n",
    "    if fps:\n",
    "        end = time.time()\n",
    "        total_time = end-start\n",
    "        print(\"%d imgs, %f seconds, average: %f fps\" % (len(imgs), total_time, len(imgs)/total_time))\n",
    "\n",
    "    with open(results_json, 'w') as f:\n",
    "        f.write(json.dumps(results, indent=4))\n",
    "\n",
    "\n",
    "    cocoGt = COCO(val_anno)\n",
    "    cocoDt = cocoGt.loadRes(results_json)\n",
    "    cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "    cocoEval.params.catIds = [class_id]\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INRIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 imgs, 13.358694 seconds, average: 21.558994 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.11s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.587\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.961\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.640\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.505\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.601\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.350\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.684\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.628\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.695\n"
     ]
    }
   ],
   "source": [
    "detection_v3(\n",
    "    val_anno=\"Dataset/HA/test_annotations.json\",\n",
    "    img_dir=\"Dataset/HA/INRIA/pos/\",\n",
    "    results_json=\"test_map/yolov3_inria.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 imgs, 11.480503 seconds, average: 25.086009 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.08s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.157\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.341\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.114\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.078\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.177\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.224\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.153\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.290\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.290\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.379\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.289\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.285\n"
     ]
    }
   ],
   "source": [
    "detection_v3(\n",
    "    val_anno=\"Dataset/HA/test_adv_annotations.json\",\n",
    "    img_dir=\"Dataset/HA/INRIA/patch_v3/\",\n",
    "    results_json=\"test_map/yolov3_inria_patchv3.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 imgs, 11.305015 seconds, average: 25.475419 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.08s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.448\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.178\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.173\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.328\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.204\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.363\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.363\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.268\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.312\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.428\n"
     ]
    }
   ],
   "source": [
    "detection_v3(\n",
    "    val_anno=\"Dataset/HA/test_adv_annotations.json\",\n",
    "    img_dir=\"Dataset/HA/INRIA/natural_v3/\",\n",
    "    results_json=\"test_map/yolov2_inria_naturalv3.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AA 攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 imgs, 39.602939 seconds, average: 25.250651 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.20s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.04s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.609\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.806\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.791\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.756\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.464\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.649\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.654\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.654\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.803\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497\n"
     ]
    }
   ],
   "source": [
    "detection_v3(\n",
    "    val_anno=\"Dataset/AA/stop_test_annotations.json\",\n",
    "    img_dir=\"Dataset/AA/imgs_s/\",\n",
    "    results_json=\"test_map/yolov3_aa_stopsign.json\",\n",
    "    class_id=11,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 imgs, 41.375729 seconds, average: 24.168759 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.35s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.736\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.952\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.881\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.637\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.830\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.787\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.806\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.806\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.728\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.884\n"
     ]
    }
   ],
   "source": [
    "detection_v3(\n",
    "    val_anno=\"Dataset/AA/person_test_annotations.json\",\n",
    "    img_dir=\"Dataset/AA/imgs_p/\",\n",
    "    results_json=\"test_map/yolov3_aa_person.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from tools.cocotools import get_classes\n",
    "from model.yolov4 import YOLOv4\n",
    "from model.decode_np import Decode\n",
    "from util_model_img import letterbox_image, image_to_tensor\n",
    "\n",
    "\n",
    "confidence = 0.5\n",
    "nms_thresh = 0.4\n",
    "num_anchors = 3\n",
    "num_classes = 80\n",
    "inp_dim = 416\n",
    "device=\"cuda\"\n",
    "device = torch.device(device)\n",
    "all_classes = get_classes(\"data/coco.names\")\n",
    "num_classes = len(all_classes)\n",
    "yolo = YOLOv4(num_classes, num_anchors)\n",
    "yolo = yolo.to(device)\n",
    "yolo.load_state_dict(torch.load(\"model_weights/pytorch_yolov4_1.pt\"))\n",
    "yolo.eval()\n",
    "model = Decode(confidence, nms_thresh, (inp_dim, inp_dim), yolo, all_classes, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "\n",
    "def detection_v4(val_anno, img_dir, results_json, class_id, CUDA=True, fps=False):\n",
    "    \n",
    "    '''\n",
    "    val_anno: annotations of validation set images\n",
    "    img_dir: dir of validation set images\n",
    "    results_json: detection result\n",
    "    class_id: HA target class - person (0) and AA target class - stop sign (11)\n",
    "    '''\n",
    "\n",
    "    file2id = dict()\n",
    "    with open(val_anno) as f:\n",
    "        dic = json.load(f)\n",
    "        images = dic['images']\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            file2id[image['file_name']] = image['id']\n",
    "\n",
    "    #imgs = glob.glob(\"%s/*.png\" % img_dir) + glob.glob(\"%s/*.jpg\" % img_dir)\n",
    "    imgs = [f for f in os.listdir(img_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    if fps:\n",
    "        start = time.time()\n",
    "\n",
    "    for img in imgs:\n",
    "\n",
    "        image_id = file2id[img]\n",
    "\n",
    "        img_cv = cv2.imread(os.path.join(img_dir, img))\n",
    "        img_h, img_w, _ = img_cv.shape\n",
    "        img_cv = letterbox_image(img_cv, [inp_dim, inp_dim]).astype(np.uint8)\n",
    "        im_dim = torch.FloatTensor((img_w, img_h)).repeat(1,2)\n",
    "        #img_ts = image_to_tensor(img_cv)\n",
    "        #print(img_cv.shape)\n",
    "        image, boxes, scores, classes = model.detect_image(img_cv, draw_image=False)\n",
    "        \n",
    "        if boxes is None:\n",
    "            pass\n",
    "        else:\n",
    "            output = np.concatenate((boxes, np.expand_dims(scores,axis=-1), np.expand_dims(classes,axis=-1)), axis=1)\n",
    "\n",
    "            im_dim = im_dim.repeat(output.shape[0], 1)\n",
    "            scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1).cpu().numpy()\n",
    "            output[:,[0,2]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1).cpu().numpy())/2\n",
    "            output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1).cpu().numpy())/2\n",
    "            output[:,0:4] /= scaling_factor\n",
    "\n",
    "            for i in range(output.shape[0]):\n",
    "\n",
    "                conf = output[i, 4]\n",
    "                cls = output[i, -1]\n",
    "                x = output[i, 0]\n",
    "                y = output[i, 1]\n",
    "                w = (output[i, 2] - output[i, 0])\n",
    "                h = (output[i, 3] - output[i, 1])\n",
    "\n",
    "                #print(image_id, cls, [xc, yc, w, h], conf)\n",
    "\n",
    "                results.append({'image_id': image_id,\n",
    "                                'category_id': cls,\n",
    "                                'bbox': [x, y, w, h],\n",
    "                                'score': conf})\n",
    "        \n",
    "    if fps:\n",
    "        end = time.time()\n",
    "        total_time = end-start\n",
    "        print(\"%d imgs, %f seconds, average: %f fps\" % (len(imgs), total_time, len(imgs)/total_time))\n",
    "\n",
    "    with open(results_json, 'w') as f:\n",
    "        f.write(json.dumps(results, indent=4))\n",
    "\n",
    "\n",
    "    cocoGt = COCO(val_anno)\n",
    "    cocoDt = cocoGt.loadRes(results_json)\n",
    "    cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "    cocoEval.params.catIds = [class_id]\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INRIA 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/lzj/Linux/PatchFilter_Submit/model/decode_np.py:194: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 imgs, 17.571758 seconds, average: 16.389937 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.12s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.626\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.948\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.710\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.496\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.648\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.361\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.702\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.708\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.645\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.721\n"
     ]
    }
   ],
   "source": [
    "detection_v4(\n",
    "    val_anno=\"Dataset/HA/test_annotations.json\",\n",
    "    img_dir=\"Dataset/HA/INRIA/pos/\",\n",
    "    results_json=\"test_map/yolov4_inria.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 imgs, 15.070274 seconds, average: 19.110469 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.10s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.239\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.473\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.236\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.110\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.366\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.391\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.391\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.395\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.328\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.462\n"
     ]
    }
   ],
   "source": [
    "detection_v4(\n",
    "    val_anno=\"Dataset/HA/test_adv_annotations.json\",\n",
    "    img_dir=\"Dataset/HA/INRIA/patch_v4/\",\n",
    "    results_json=\"test_map/yolov4_inria_patchv4.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 imgs, 14.958608 seconds, average: 19.253129 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.13s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.810\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.532\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.179\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.472\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.558\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.313\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.606\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.606\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.463\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.568\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.657\n"
     ]
    }
   ],
   "source": [
    "detection_v4(\n",
    "    val_anno=\"Dataset/HA/test_adv_annotations.json\",\n",
    "    img_dir=\"Dataset/HA/INRIA/natural_v4/\",\n",
    "    results_json=\"test_map/yolov4_inria_naturalv4.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AA 攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 imgs, 51.884302 seconds, average: 19.273652 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.30s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.08s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.927\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.927\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.813\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.792\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.837\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.838\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.838\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.853\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.826\n"
     ]
    }
   ],
   "source": [
    "detection_v4(\n",
    "    val_anno=\"Dataset/AA/stop_test_annotations.json\",\n",
    "    img_dir=\"Dataset/AA/imgs_s/\",\n",
    "    results_json=\"test_map/yolov4_aa_stopsign.json\",\n",
    "    class_id=11,\n",
    "    fps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 imgs, 52.520471 seconds, average: 19.040195 fps\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.27s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.06s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.734\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.885\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.876\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.652\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.843\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.798\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.841\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.841\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.778\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.903\n"
     ]
    }
   ],
   "source": [
    "detection_v4(\n",
    "    val_anno=\"Dataset/AA/person_test_annotations.json\",\n",
    "    img_dir=\"Dataset/AA/imgs_p/\",\n",
    "    results_json=\"test_map/yolov4_aa_person.json\",\n",
    "    class_id=0,\n",
    "    fps=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae3b0402f9dd51eaa3f0a02741215eae93554903d09a4648ec36843d0772e640"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
